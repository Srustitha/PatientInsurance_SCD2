# Patient Demographics & Insurance History Management Using SCD-2

This repository contains Databricks notebooks that implement a complete end-to-end data ingestion and transformation pipeline using Apache Spark, Delta Lake, and Slowly Changing Dimension (SCD) Type 2 methodology, following the industry-standard Medallion Architecture. The pipeline begins by ingesting raw source data from Amazon S3 into the Bronze (Landing) layer using Spark and Databricks Auto Loader, where audit metadata such as ingestion timestamps and source file names are added to ensure traceability and fault tolerance (s3_to_landing). The data is then moved into the Silver (Base/Staging) layer, where it is cleaned, standardized, and prepared for analytics through data type corrections, null handling, and deduplication, resulting in reliable and consistent base tables (Staging_and_SCD2). In the final stage, the Gold layer implements SCD Type 2 logic by comparing incoming Silver data with existing records, detecting attribute changes, expiring outdated records, and inserting new versions while preserving complete historical data using Delta Lake (Base_table). Together, these notebooks demonstrate a scalable, auditable, and production-ready data pipeline designed to support accurate historical analysis and downstream reporting.
 
